{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil.parser import parse\n",
    "import email\n",
    "import matplotlib.image as mpimg\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailbox\n",
    "\n",
    "# Constants to change\n",
    "PROJECT_PATH = 'enron'\n",
    "\n",
    "DIRNAME = os.path.dirname(os.getcwd())+os.sep+PROJECT_PATH+os.sep\n",
    "MBOX_PATH = DIRNAME+'enron.mbox'\n",
    "DATA = DIRNAME+'maildir'+os.sep\n",
    "SAMPLE = DIRNAME+'sample'+os.sep+'1'+os.sep\n",
    "MBOX = mailbox.mbox(MBOX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maildir_to_mbox():\n",
    "    with open(MBOX_PATH, 'w') as mbox:   \n",
    "        for (root, dirs, file_names) in os.walk(DATA):\n",
    "            if root.split(os.sep)[-1].lower() != 'inbox':\n",
    "                continue\n",
    "\n",
    "            # Process each message in 'inbox'\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                message_text = open(file_path).read()\n",
    "                msg = email.message_from_string(message_text)\n",
    "                mbox.write(msg.as_string(unixfrom=True) + \"\\n\\n\")\n",
    "\n",
    "    return mbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maildir_to_mbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sample creation.\"\"\"\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy\n",
    "\n",
    "\n",
    "def create_sample(sample_size, number_of_sample):\n",
    "    \"\"\"Create a sample of emails among all the emails.\"\"\"\n",
    "    print(f\"Creation of {number_of_sample} samples each contening {sample_size} emails.\")\n",
    "\n",
    "# Definition of the different directories.\n",
    "\n",
    "    dirname = os.getcwd()\n",
    "    mailDir = os.path.join(dirname, 'maildir')\n",
    "\n",
    "# Counting emails.\n",
    "\n",
    "    mails_count = sum(len(files) for _, _, files in os.walk(mailDir))\n",
    "    print(f\"The maildir folder contains {mails_count} emails.\")\n",
    "\n",
    "# Cleaning the target file.\n",
    "\n",
    "    shutil.rmtree(os.path.join(dirname, 'sample'))\n",
    "\n",
    "    for sample in range(1, number_of_sample+1):\n",
    "        print(f\"Sample number {sample}.\")\n",
    "        sampleDir = os.path.join(dirname, 'sample', str(sample))\n",
    "\n",
    "        if not os.path.exists(sampleDir):\n",
    "            os.makedirs(sampleDir)\n",
    "\n",
    "        # Draw random emails and copy them to the sample folder.\n",
    "        random_list = numpy.random.randint(1, mails_count+1, sample_size)\n",
    "        print(len(random_list))\n",
    "        id_mail = 1\n",
    "        for repertory, sub_repertory, files in os.walk(mailDir):\n",
    "            for f in files:\n",
    "                if id_mail in random_list:\n",
    "                    shutil.copy(os.path.join(repertory, f), sampleDir)\n",
    "                    os.rename(os.path.join(sampleDir, f), os.path.join(\n",
    "                        sampleDir, str(id_mail)))\n",
    "                id_mail += 1\n",
    "        print(f\"Creation of the sample {sample} successfully completed.\")\n",
    "        print(\n",
    "            f\"{sample_size} random mails have been copied to target repertory.\"\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"OK - {number_of_sample} samples of {sample_size} emails created in \"\n",
    "        \"target repertory.\")\n",
    "\n",
    "\n",
    "create_sample(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_mbox():\n",
    "    with open(MBOX, 'w') as mbox:   \n",
    "        for (root, dirs, file_names) in os.walk(SAMPLE):\n",
    "            # Process each message \n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                message_text = open(file_path).read()\n",
    "                msg = email.message_from_string(message_text)\n",
    "                mbox.write(msg.as_string(unixfrom=True) + \"\\n\\n\")\n",
    "\n",
    "    return mbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_mbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    \"\"\"Clean the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (dict): dictionnary to clean\n",
    "\n",
    "    Returns:\n",
    "        dict: Enron corpus cleaned dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    cols_to_keep = ['Date', 'From', 'To', 'Cc', 'Bcc', 'Subject', 'Body']\n",
    "    df = df[cols_to_keep]\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.drop_duplicates(subset=['Date', 'From', 'To'])\n",
    "    df.dropna(subset = [\"Date\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def mbox_to_df():\n",
    "    \"\"\"Convert the mailbox to a dataframe.\n",
    "\n",
    "    Returns:\n",
    "        dict: Enron corpus sample dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    mbox_dict = {}\n",
    "\n",
    "    for i, msg in enumerate(MBOX):\n",
    "        mbox_dict[i] = {}\n",
    "        for header in msg.keys():\n",
    "            mbox_dict[i][header] = msg[header]\n",
    "\n",
    "        mbox_dict[i]['Body'] = msg.get_payload().replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "\n",
    "    df = pd.DataFrame.from_dict(mbox_dict, orient='index')\n",
    "    df = data_cleaning(df)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    df.to_csv('emails.csv', sep='|')\n",
    "    # df.to_csv('sample.csv', sep='|')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = mbox_to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Complete dataframe\n",
    "df = pd.read_csv('emails.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sampled dataframe\n",
    "df = pd.read_csv('sample.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Day'] = df['Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reduce the sample period\n",
    "# looks like the total number of emails really ramped up in 2000 and 2001\n",
    "indices = (df['Year'] >= 1997) & (df['Year'] <= 2004)\n",
    "plt.figure(figsize = (10,6))\n",
    "figure1 = df.loc[indices].groupby('Year')['Body'].count().plot(title='Network messages traffic between 1997 and 2004')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the sample period\n",
    "# looks like the total number of emails really ramped up in october 2001 (date of bankruptcy)\n",
    "indices = (df['Year'] >= 2000) & (df['Year'] <= 2001)\n",
    "plt.figure(figsize = (10,6))\n",
    "df['month_year'] = pd.to_datetime(df['Date'], utc=True).dt.to_period('M')\n",
    "figure2 = df.loc[indices].groupby('month_year')['Body'].count().plot(kind='bar', title='Network messages traffic between 2000 and 2001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[(df['Year'] >= 2000) & (df['Year'] <= 2001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create the graph\n",
    "G_symmetric = nx.Graph()\n",
    "\n",
    "for index, mail in df.iterrows():\n",
    "    mail_to=mail[2].split(sep=\",\")\n",
    "\n",
    "    for adress in mail_to:\n",
    "        # --- Tester si le node existe et si il existe ajouter +1 au weight\n",
    "        G_symmetric.add_edge(mail[1],adress,weight=1)\n",
    "\n",
    "pos = nx.spring_layout(G_symmetric)\n",
    "betCent = nx.betweenness_centrality(G_symmetric, normalized=True, endpoints=True)\n",
    "node_color = [20000.0 * G_symmetric.degree(v) for v in G_symmetric]\n",
    "node_size =  [v * 10000 for v in betCent.values()]\n",
    "plt.figure(figsize=(50, 50))\n",
    "nx.draw_networkx(G_symmetric, pos=pos, with_labels=True,\n",
    "                 node_color=node_color,\n",
    "                 node_size=node_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "stopwords = ENGLISH_STOP_WORDS.union(['ect', 'hou', 'com'])\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "# Keep only body message\n",
    "cols_to_keep = ['Body']\n",
    "df = df[cols_to_keep]\n",
    "\n",
    "X = vectorizer.fit_transform(df['Body'].values.astype(str))\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "model = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print('Cluster ', i)\n",
    "    for ind in order_centroids[i, :100]:\n",
    "        print(terms[ind])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('Prediction')\n",
    "email_to_test = 'Enter email here'\n",
    "X3 = vectorizer.transform([email_to_test])\n",
    "predicted = model.predict(X3)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For all emails\n",
    "X2 = vectorizer.fit_transform(df['Body'].values.astype(str))\n",
    "\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X2)\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(3):\n",
    "    print('Cluster ', i)\n",
    "    for ind in order_centroids[i, :100]:\n",
    "        print(terms[ind])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "with open('sample.csv', 'r') as csvfile :\n",
    "    mail = csv.reader(csvfile, delimiter='|')\n",
    "    tokens=''\n",
    "    for message in mail:\n",
    "        tokens=tokens + message[6]\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"[a-zA-Z]{3,}\")\n",
    "tokenized_messages = [t.lower() for t in tokenizer.tokenize(tokens)]\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_tokenized_messages = [word for word in tokenized_messages if word not in stopwords]\n",
    "without_words=['hour', 'codesite', 'hourahead', 'date', 'request', 'access', 'start']\n",
    "filtered_tokenized_messages = [word for word in filtered_tokenized_messages if word not in without_words]\n",
    "fdist = FreqDist(filtered_tokenized_messages)\n",
    "common = fdist.most_common(100)\n",
    "print(fdist)\n",
    "print(common)\n",
    "\n",
    "trigram_collocation = TrigramCollocationFinder.from_words(filtered_tokenized_messages)\n",
    "trigram_collocation.nbest(TrigramAssocMeasures.likelihood_ratio, 100)\n",
    "\n",
    "\n",
    "import pandas_profiling\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def wordFromCSV(src, word):\n",
    "    \"\"\"Create a dataframe of the mails containing the target word.\n",
    "    \n",
    "    Args:\n",
    "        src(csv): source of mails to explore.\n",
    "        word(str): target word to find.\n",
    "\n",
    "    Returns:\n",
    "        df:dataframe\n",
    "    \"\"\"\n",
    "    csv.field_size_limit(100000000)\n",
    "    data=[]\n",
    "    target=word.lower()\n",
    "\n",
    "    with open(src, 'r') as csvfile:\n",
    "        mails = csv.reader(csvfile, delimiter='|')\n",
    "        for message in mails:\n",
    "            body_message=message[7].lower()\n",
    "            if body_message.__contains__(target)==True:\n",
    "                temp=[\n",
    "                    message[1],\n",
    "                    message[2],\n",
    "                    message[3],\n",
    "                    message[4],\n",
    "                    message[5],\n",
    "                    message[6],\n",
    "                    message[7]]\n",
    "                    \n",
    "                data.append(temp)\n",
    "                df = pd.DataFrame(data, columns=['Date',  'From',  'To', 'Cc',  'Bcc',  'Subject', 'Body']) \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = wordFromCSV('sample.csv', 'work')\n",
    "print(\"Le fichier a \" + str(df.shape[0]) + \" lignes et \" + str(df.shape[1]) + \" colonnes\")\n",
    "\n",
    "pandas_profiling.ProfileReport(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}