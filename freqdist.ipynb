{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598359208534",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "100000000"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "csv.field_size_limit(100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('sample.csv', 'r') as csvfile :\n",
    "    mail = csv.reader(csvfile, delimiter='|')\n",
    "    tokens=''\n",
    "    for message in mail:\n",
    "        tokens=tokens + message[6]\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "#tokenizer.tokenize(tokens)\n",
    "#tokens = str([t.lower() for t in tokenizer.tokenize(tokens)])\n",
    "tokenized_messages=nltk.word_tokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<FreqDist with 13763 samples and 36835 outcomes>\n[(':', 4138), ('-', 842), (',', 557), ('!', 433), (';', 307), (')', 295), ('(', 294), (\"'s\", 266), ('&', 249), ('Re', 188), ('Enron', 182), ('FW', 177), ('HourAhead', 170), ('?', 161), ('.', 158), ('--', 151), ('>', 148), ('<', 147), ('Date', 146), ('hour', 144), ('Energy', 136), ('Power', 124), ('Gas', 123), ('CODESITE', 114), ('RE', 113), ('...', 105), ('Start', 95), ('Report', 93), ('New', 87), (\"''\", 86), ('Meeting', 86), ('#', 82), ('/', 74), ('2001', 73), ('Credit', 71), ('Conference', 70), ('@', 65), ('Request', 63), ('``', 60), ('Trading', 60), ('EOL', 59), ('Daily', 55), (']', 53), ('2000', 52), ('[', 52), ('Deal', 51), ('Your', 50), ('Access', 50), ('Update', 47), ('The', 47), ('Schedule', 45), ('Market', 44), ('May', 43), ('A', 43), ('October', 42), ('2', 42), ('California', 42), ('FERC', 42), ('Agreement', 42), ('Notification', 41), ('Call', 40), ('Fwd', 40), ('new', 40), ('Master', 40), ('E', 39), ('June', 39), ('$', 39), ('Management', 39), ('Lay', 39), ('FOR', 38), ('ISDA', 38), ('December', 37), ('Outage', 37), ('I', 36), ('Summary', 36), ('Ken', 35), ('Risk', 35), ('GE', 35), ('Weekly', 34), ('News', 34), ('Contract', 32), ('ENA', 32), ('Stock', 32), ('West', 32), ('1', 31), ('Data', 31), ('AgreementRe', 31), ('Services', 30), ('Letter', 30), ('Legal', 30), ('Order', 29), ('April', 29), ('Fw', 29), ('Price', 29), ('Expense', 28), ('March', 28), ('Crawler', 28), ('OF', 27), ('In', 27), ('Financial', 27)]\n"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_tokenized_messages = [word for word in tokenized_messages if word not in stopwords]\n",
    "fdist = FreqDist(filtered_tokenized_messages)\n",
    "common = fdist.most_common(100)\n",
    "#print(tokenized_messages)\n",
    "print(fdist)\n",
    "print(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}